{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fish Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Linux User / Linux 用户"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en_US.UTF-8'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 7 files:   0%|                                   | 0/7 [00:00<?, ?it/s]Downloading 'model.pth' to 'checkpoints/fish-speech-1.5/.cache/huggingface/download/YT0Y2lJH9mHYafdr2d9j82hXvzY=.918dc960372cc1b77bbafb14c48ef7a1634ecf75d4eb85b78607223b780d6001.incomplete'\n",
      "Downloading '.gitattributes' to 'checkpoints/fish-speech-1.5/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete'\n",
      "Downloading 'special_tokens.json' to 'checkpoints/fish-speech-1.5/.cache/huggingface/download/Pdr1pnDFqf3r8xSTD-lPnaCpeRA=.db54e3cccbbaa1106ba8d56e810dffd42e325ab0.incomplete'\n",
      "Downloading 'firefly-gan-vq-fsq-8x1024-21hz-generator.pth' to 'checkpoints/fish-speech-1.5/.cache/huggingface/download/Khmizewsuzbxb3XfvhhbrTGaoLE=.01b81dbf753224a156c3fe139b88bf0b9a0f54b11bee864f95e66511c3ccd754.incomplete'\n",
      "Downloading 'config.json' to 'checkpoints/fish-speech-1.5/.cache/huggingface/download/8_PA_wEVGiVa2goH2H4KQOQpvVY=.3f8edf91f7a0b152e5f8c30fd412c5d7e22020b5.incomplete'\n",
      "Downloading 'README.md' to 'checkpoints/fish-speech-1.5/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.0d8d6dec1dd7f9e8739ddadb33b8c1b2cc5acc65.incomplete'\n",
      "\n",
      ".gitattributes:   0%|                               | 0.00/1.52k [00:00<?, ?B/s]\u001b[A\n",
      ".gitattributes: 100%|██████████████████████| 1.52k/1.52k [00:00<00:00, 2.72MB/s]\u001b[A\n",
      "Download complete. Moving file to checkpoints/fish-speech-1.5/.gitattributes\n",
      "special_tokens.json: 100%|█████████████████| 31.0k/31.0k [00:00<00:00, 54.9MB/s]\n",
      "Download complete. Moving file to checkpoints/fish-speech-1.5/special_tokens.json\n",
      "Fetching 7 files:  14%|███▊                       | 1/7 [00:00<00:02,  2.23it/s]\n",
      "config.json: 100%|█████████████████████████████| 697/697 [00:00<00:00, 5.62MB/s]\u001b[A\n",
      "Download complete. Moving file to checkpoints/fish-speech-1.5/config.json\n",
      "\n",
      "README.md: 100%|███████████████████████████| 1.68k/1.68k [00:00<00:00, 9.08MB/s]\u001b[A\n",
      "Download complete. Moving file to checkpoints/fish-speech-1.5/README.md\n",
      "\n",
      "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:   0%|   | 0.00/189M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "model.pth:   0%|                                    | 0.00/1.28G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:   2%|▍                           | 21.0M/1.28G [00:00<00:06, 207MB/s]\u001b[A\u001b[ADownloading 'tokenizer.tiktoken' to 'checkpoints/fish-speech-1.5/.cache/huggingface/download/zENsYUfT6EG2Nj68LEJ8oOfAxB8=.21dcfcb37df8da533b2d4fe0b867472f04cda62e.incomplete'\n",
      "\n",
      "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  17%|▏| 31.5M/189M [00:00<00:00, 24\u001b[A\n",
      "\n",
      "model.pth:   5%|█▍                          | 62.9M/1.28G [00:00<00:03, 306MB/s]\u001b[A\u001b[A\n",
      "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  39%|▍| 73.4M/189M [00:00<00:00, 31\u001b[A\n",
      "\n",
      "model.pth:   9%|██▌                          | 115M/1.28G [00:00<00:03, 366MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "tokenizer.tiktoken:   0%|                           | 0.00/1.70M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  61%|▌| 115M/189M [00:00<00:00, 336\u001b[A\n",
      "\n",
      "model.pth:  12%|███▌                         | 157M/1.28G [00:00<00:03, 361MB/s]\u001b[A\u001b[A\n",
      "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  83%|▊| 157M/189M [00:00<00:00, 355\u001b[A\n",
      "\n",
      "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth: 100%|█| 189M/189M [00:00<00:00, 327\u001b[A\u001b[A\n",
      "Download complete. Moving file to checkpoints/fish-speech-1.5/firefly-gan-vq-fsq-8x1024-21hz-generator.pth\n",
      "Fetching 7 files:  57%|███████████████▍           | 4/7 [00:01<00:00,  3.82it/s]\n",
      "\n",
      "model.pth:  19%|█████▍                       | 241M/1.28G [00:00<00:02, 385MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  22%|██████▍                      | 283M/1.28G [00:00<00:02, 363MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  25%|███████▍                     | 325M/1.28G [00:00<00:02, 345MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  29%|████████▎                    | 367M/1.28G [00:01<00:02, 364MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  33%|█████████▌                   | 419M/1.28G [00:01<00:02, 385MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "tokenizer.tiktoken: 100%|██████████████████| 1.70M/1.70M [00:00<00:00, 2.05MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to checkpoints/fish-speech-1.5/tokenizer.tiktoken\n",
      "\n",
      "\n",
      "model.pth:  36%|██████████▍                  | 461M/1.28G [00:01<00:02, 373MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  39%|███████████▍                 | 503M/1.28G [00:01<00:02, 359MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  43%|████████████▍                | 545M/1.28G [00:01<00:02, 363MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  46%|█████████████▎               | 587M/1.28G [00:01<00:01, 360MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  49%|██████████████▎              | 629M/1.28G [00:01<00:01, 332MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  53%|███████████████▎             | 671M/1.28G [00:01<00:01, 341MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  56%|████████████████▏            | 713M/1.28G [00:02<00:01, 353MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  59%|█████████████████▏           | 755M/1.28G [00:02<00:01, 344MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  62%|██████████████████           | 797M/1.28G [00:02<00:01, 348MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  66%|███████████████████          | 839M/1.28G [00:02<00:01, 361MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  69%|████████████████████         | 881M/1.28G [00:02<00:01, 373MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  73%|█████████████████████▏       | 933M/1.28G [00:02<00:00, 392MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  76%|██████████████████████▏      | 975M/1.28G [00:02<00:00, 393MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  80%|██████████████████████▎     | 1.02G/1.28G [00:02<00:00, 399MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  83%|███████████████████████▏    | 1.06G/1.28G [00:02<00:00, 403MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  87%|████████████████████████▍   | 1.11G/1.28G [00:03<00:00, 411MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  91%|█████████████████████████▌  | 1.16G/1.28G [00:03<00:00, 415MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  95%|██████████████████████████▍ | 1.21G/1.28G [00:03<00:00, 415MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth: 100%|████████████████████████████| 1.28G/1.28G [00:03<00:00, 375MB/s]\u001b[A\u001b[A\n",
      "Download complete. Moving file to checkpoints/fish-speech-1.5/model.pth\n",
      "Fetching 7 files: 100%|███████████████████████████| 7/7 [00:03<00:00,  1.78it/s]\n",
      "/home/ubuntu/Projects/nisarg/fish-speech/checkpoints/fish-speech-1.5\n"
     ]
    }
   ],
   "source": [
    "# For Chinese users, you probably want to use mirror to accelerate downloading\n",
    "# !set HF_ENDPOINT=https://hf-mirror.com\n",
    "# !export HF_ENDPOINT=https://hf-mirror.com \n",
    "\n",
    "!huggingface-cli download fishaudio/fish-speech-1.5 --local-dir checkpoints/fish-speech-1.5/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WebUI Inference\n",
    "\n",
    "> You can use --compile to fuse CUDA kernels for faster inference (10x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-09 07:57:51.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mLoading Llama model...\u001b[0m\n",
      "\u001b[32m2025-06-09 07:58:06.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m681\u001b[0m - \u001b[1mRestored model from checkpoint\u001b[0m\n",
      "\u001b[32m2025-06-09 07:58:06.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m687\u001b[0m - \u001b[1mUsing DualARTransformer\u001b[0m\n",
      "\u001b[32m2025-06-09 07:58:06.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m695\u001b[0m - \u001b[1mCompiling function...\u001b[0m\n",
      "\u001b[32m2025-06-09 07:58:08.130\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mLoading VQ-GAN model...\u001b[0m\n",
      "/home/bhavsar_nisarg_nobroker_in/miniconda3/envs/fish-speech/lib/python3.11/site-packages/vector_quantize_pytorch/vector_quantize_pytorch.py:445: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/bhavsar_nisarg_nobroker_in/miniconda3/envs/fish-speech/lib/python3.11/site-packages/vector_quantize_pytorch/vector_quantize_pytorch.py:630: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/bhavsar_nisarg_nobroker_in/miniconda3/envs/fish-speech/lib/python3.11/site-packages/vector_quantize_pytorch/finite_scalar_quantization.py:147: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/bhavsar_nisarg_nobroker_in/miniconda3/envs/fish-speech/lib/python3.11/site-packages/vector_quantize_pytorch/lookup_free_quantization.py:209: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "\u001b[32m2025-06-09 07:58:09.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.vqgan.inference\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m46\u001b[0m - \u001b[1mLoaded model: <All keys matched successfully>\u001b[0m\n",
      "\u001b[32m2025-06-09 07:58:09.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m71\u001b[0m - \u001b[1mDecoder model loaded, warming up...\u001b[0m\n",
      "\u001b[32m2025-06-09 07:58:09.949\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Hello world.\u001b[0m\n",
      "\u001b[32m2025-06-09 07:58:09.949\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/1 of sample 1/1\u001b[0m\n",
      "  0%|                                                  | 0/1023 [00:00<?, ?it/s]/home/bhavsar_nisarg_nobroker_in/miniconda3/envs/fish-speech/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "W0609 07:59:48.850000 139265746921024 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.\n",
      "  0%|                                      | 1/1023 [03:45<64:00:53, 225.49s/it]/home/bhavsar_nisarg_nobroker_in/miniconda3/envs/fish-speech/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "  0%|                                       | 2/1023 [03:46<26:29:52, 93.43s/it]/home/bhavsar_nisarg_nobroker_in/miniconda3/envs/fish-speech/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "  2%|▉                                      | 23/1023 [03:46<2:44:16,  9.86s/it]\n",
      "\u001b[32m2025-06-09 08:01:57.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 227.55 seconds\u001b[0m\n",
      "\u001b[32m2025-06-09 08:01:57.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 25 tokens in 227.55 seconds, 0.11 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-09 08:01:57.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 0.07 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-09 08:01:57.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.22 GB\u001b[0m\n",
      "\u001b[32m2025-06-09 08:01:57.520\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 24])\u001b[0m\n",
      "\u001b[32m2025-06-09 08:01:59.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m98\u001b[0m - \u001b[1mWarming up done, launching the web UI...\u001b[0m\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n",
      "\u001b[32m2025-06-09 08:03:05.140\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: ನೀವಿನಾ query solution ಅಂತೆ ಆಸೆ ಇಟ್ಟಿರಾ, ನಾವು ಆದ್ಯತೆಕ್ರಮಲ್ಲಿ process ಮಾಡುತ್ತೇವೆ।\u001b[0m\n",
      "\u001b[32m2025-06-09 08:03:05.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/1 of sample 1/1\u001b[0m\n",
      "  0%|                                                  | 0/8023 [00:00<?, ?it/s]/home/bhavsar_nisarg_nobroker_in/miniconda3/envs/fish-speech/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "  6%|██▎                                    | 480/8023 [00:04<01:15, 100.25it/s]\n",
      "\u001b[32m2025-06-09 08:03:10.012\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 4.87 seconds\u001b[0m\n",
      "\u001b[32m2025-06-09 08:03:10.012\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 482 tokens in 4.87 seconds, 98.95 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-09 08:03:10.012\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 63.13 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-09 08:03:10.013\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.26 GB\u001b[0m\n",
      "\u001b[32m2025-06-09 08:03:10.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 481])\u001b[0m\n",
      "/home/bhavsar_nisarg_nobroker_in/miniconda3/envs/fish-speech/lib/python3.11/site-packages/gradio/processing_utils.py:753: UserWarning: Trying to convert audio automatically from float32 to 16-bit int format.\n",
      "  warnings.warn(warning.format(data.dtype))\n"
     ]
    }
   ],
   "source": [
    "!python tools/run_webui.py \\\n",
    "    --llama-checkpoint-path checkpoints/fish-speech-1.5-guj-lora \\\n",
    "    --decoder-checkpoint-path checkpoints/fish-speech-1.5/firefly-gan-vq-fsq-8x1024-21hz-generator.pth \\\n",
    "    --compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break-down CLI Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Encode reference audio: / 从语音生成 prompt: \n",
    "\n",
    "You should get a `fake.npy` file.\n",
    "\n",
    "你应该能得到一个 `fake.npy` 文件."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "## Enter the path to the audio file here\n",
    "src_audio = r\"D:\\PythonProject\\vo_hutao_draw_appear.wav\"\n",
    "\n",
    "!python fish_speech/models/vqgan/inference.py \\\n",
    "    -i {src_audio} \\\n",
    "    --checkpoint-path \"checkpoints/fish-speech-1.5/firefly-gan-vq-fsq-8x1024-21hz-generator.pth\"\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "audio = Audio(filename=\"fake.wav\")\n",
    "display(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate semantic tokens from text: / 从文本生成语义 token:\n",
    "\n",
    "> This command will create a codes_N file in the working directory, where N is an integer starting from 0.\n",
    "\n",
    "> You may want to use `--compile` to fuse CUDA kernels for faster inference (~30 tokens/second -> ~300 tokens/second).\n",
    "\n",
    "> 该命令会在工作目录下创建 codes_N 文件, 其中 N 是从 0 开始的整数.\n",
    "\n",
    "> 您可以使用 `--compile` 来融合 cuda 内核以实现更快的推理 (~30 tokens/秒 -> ~300 tokens/秒)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python fish_speech/models/text2semantic/inference.py \\\n",
    "    --text \"hello world\" \\\n",
    "    --prompt-text \"The text corresponding to reference audio\" \\\n",
    "    --prompt-tokens \"fake.npy\" \\\n",
    "    --checkpoint-path \"checkpoints/fish-speech-1.5\" \\\n",
    "    --num-samples 2\n",
    "    # --compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate speech from semantic tokens: / 从语义 token 生成人声:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python fish_speech/models/vqgan/inference.py \\\n",
    "    -i \"codes_0.npy\" \\\n",
    "    --checkpoint-path \"checkpoints/fish-speech-1.5/firefly-gan-vq-fsq-8x1024-21hz-generator.pth\"\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "audio = Audio(filename=\"fake.wav\")\n",
    "display(audio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fish-speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
