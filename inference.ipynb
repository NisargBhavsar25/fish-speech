{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fish Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Linux User / Linux 用户"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en_US.UTF-8'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 7 files:   0%|                                   | 0/7 [00:00<?, ?it/s]Downloading 'model.pth' to 'checkpoints/fish-speech-1.5/.cache/huggingface/download/YT0Y2lJH9mHYafdr2d9j82hXvzY=.918dc960372cc1b77bbafb14c48ef7a1634ecf75d4eb85b78607223b780d6001.incomplete'\n",
      "Downloading '.gitattributes' to 'checkpoints/fish-speech-1.5/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete'\n",
      "Downloading 'special_tokens.json' to 'checkpoints/fish-speech-1.5/.cache/huggingface/download/Pdr1pnDFqf3r8xSTD-lPnaCpeRA=.db54e3cccbbaa1106ba8d56e810dffd42e325ab0.incomplete'\n",
      "Downloading 'firefly-gan-vq-fsq-8x1024-21hz-generator.pth' to 'checkpoints/fish-speech-1.5/.cache/huggingface/download/Khmizewsuzbxb3XfvhhbrTGaoLE=.01b81dbf753224a156c3fe139b88bf0b9a0f54b11bee864f95e66511c3ccd754.incomplete'\n",
      "Downloading 'config.json' to 'checkpoints/fish-speech-1.5/.cache/huggingface/download/8_PA_wEVGiVa2goH2H4KQOQpvVY=.3f8edf91f7a0b152e5f8c30fd412c5d7e22020b5.incomplete'\n",
      "Downloading 'README.md' to 'checkpoints/fish-speech-1.5/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.0d8d6dec1dd7f9e8739ddadb33b8c1b2cc5acc65.incomplete'\n",
      "\n",
      ".gitattributes:   0%|                               | 0.00/1.52k [00:00<?, ?B/s]\u001b[A\n",
      ".gitattributes: 100%|██████████████████████| 1.52k/1.52k [00:00<00:00, 2.72MB/s]\u001b[A\n",
      "Download complete. Moving file to checkpoints/fish-speech-1.5/.gitattributes\n",
      "special_tokens.json: 100%|█████████████████| 31.0k/31.0k [00:00<00:00, 54.9MB/s]\n",
      "Download complete. Moving file to checkpoints/fish-speech-1.5/special_tokens.json\n",
      "Fetching 7 files:  14%|███▊                       | 1/7 [00:00<00:02,  2.23it/s]\n",
      "config.json: 100%|█████████████████████████████| 697/697 [00:00<00:00, 5.62MB/s]\u001b[A\n",
      "Download complete. Moving file to checkpoints/fish-speech-1.5/config.json\n",
      "\n",
      "README.md: 100%|███████████████████████████| 1.68k/1.68k [00:00<00:00, 9.08MB/s]\u001b[A\n",
      "Download complete. Moving file to checkpoints/fish-speech-1.5/README.md\n",
      "\n",
      "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:   0%|   | 0.00/189M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "model.pth:   0%|                                    | 0.00/1.28G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:   2%|▍                           | 21.0M/1.28G [00:00<00:06, 207MB/s]\u001b[A\u001b[ADownloading 'tokenizer.tiktoken' to 'checkpoints/fish-speech-1.5/.cache/huggingface/download/zENsYUfT6EG2Nj68LEJ8oOfAxB8=.21dcfcb37df8da533b2d4fe0b867472f04cda62e.incomplete'\n",
      "\n",
      "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  17%|▏| 31.5M/189M [00:00<00:00, 24\u001b[A\n",
      "\n",
      "model.pth:   5%|█▍                          | 62.9M/1.28G [00:00<00:03, 306MB/s]\u001b[A\u001b[A\n",
      "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  39%|▍| 73.4M/189M [00:00<00:00, 31\u001b[A\n",
      "\n",
      "model.pth:   9%|██▌                          | 115M/1.28G [00:00<00:03, 366MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "tokenizer.tiktoken:   0%|                           | 0.00/1.70M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  61%|▌| 115M/189M [00:00<00:00, 336\u001b[A\n",
      "\n",
      "model.pth:  12%|███▌                         | 157M/1.28G [00:00<00:03, 361MB/s]\u001b[A\u001b[A\n",
      "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  83%|▊| 157M/189M [00:00<00:00, 355\u001b[A\n",
      "\n",
      "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth: 100%|█| 189M/189M [00:00<00:00, 327\u001b[A\u001b[A\n",
      "Download complete. Moving file to checkpoints/fish-speech-1.5/firefly-gan-vq-fsq-8x1024-21hz-generator.pth\n",
      "Fetching 7 files:  57%|███████████████▍           | 4/7 [00:01<00:00,  3.82it/s]\n",
      "\n",
      "model.pth:  19%|█████▍                       | 241M/1.28G [00:00<00:02, 385MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  22%|██████▍                      | 283M/1.28G [00:00<00:02, 363MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  25%|███████▍                     | 325M/1.28G [00:00<00:02, 345MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  29%|████████▎                    | 367M/1.28G [00:01<00:02, 364MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  33%|█████████▌                   | 419M/1.28G [00:01<00:02, 385MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "tokenizer.tiktoken: 100%|██████████████████| 1.70M/1.70M [00:00<00:00, 2.05MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to checkpoints/fish-speech-1.5/tokenizer.tiktoken\n",
      "\n",
      "\n",
      "model.pth:  36%|██████████▍                  | 461M/1.28G [00:01<00:02, 373MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  39%|███████████▍                 | 503M/1.28G [00:01<00:02, 359MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  43%|████████████▍                | 545M/1.28G [00:01<00:02, 363MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  46%|█████████████▎               | 587M/1.28G [00:01<00:01, 360MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  49%|██████████████▎              | 629M/1.28G [00:01<00:01, 332MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  53%|███████████████▎             | 671M/1.28G [00:01<00:01, 341MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  56%|████████████████▏            | 713M/1.28G [00:02<00:01, 353MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  59%|█████████████████▏           | 755M/1.28G [00:02<00:01, 344MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  62%|██████████████████           | 797M/1.28G [00:02<00:01, 348MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  66%|███████████████████          | 839M/1.28G [00:02<00:01, 361MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  69%|████████████████████         | 881M/1.28G [00:02<00:01, 373MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  73%|█████████████████████▏       | 933M/1.28G [00:02<00:00, 392MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  76%|██████████████████████▏      | 975M/1.28G [00:02<00:00, 393MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  80%|██████████████████████▎     | 1.02G/1.28G [00:02<00:00, 399MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  83%|███████████████████████▏    | 1.06G/1.28G [00:02<00:00, 403MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  87%|████████████████████████▍   | 1.11G/1.28G [00:03<00:00, 411MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  91%|█████████████████████████▌  | 1.16G/1.28G [00:03<00:00, 415MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth:  95%|██████████████████████████▍ | 1.21G/1.28G [00:03<00:00, 415MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.pth: 100%|████████████████████████████| 1.28G/1.28G [00:03<00:00, 375MB/s]\u001b[A\u001b[A\n",
      "Download complete. Moving file to checkpoints/fish-speech-1.5/model.pth\n",
      "Fetching 7 files: 100%|███████████████████████████| 7/7 [00:03<00:00,  1.78it/s]\n",
      "/home/ubuntu/Projects/nisarg/fish-speech/checkpoints/fish-speech-1.5\n"
     ]
    }
   ],
   "source": [
    "# For Chinese users, you probably want to use mirror to accelerate downloading\n",
    "# !set HF_ENDPOINT=https://hf-mirror.com\n",
    "# !export HF_ENDPOINT=https://hf-mirror.com \n",
    "\n",
    "!huggingface-cli download fishaudio/fish-speech-1.5 --local-dir checkpoints/fish-speech-1.5/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WebUI Inference\n",
    "\n",
    "> You can use --compile to fuse CUDA kernels for faster inference (10x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-20 05:29:07.307\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mLoading Llama model...\u001b[0m\n",
      "\u001b[32m2025-06-20 05:29:19.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m681\u001b[0m - \u001b[1mRestored model from checkpoint\u001b[0m\n",
      "\u001b[32m2025-06-20 05:29:19.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m687\u001b[0m - \u001b[1mUsing DualARTransformer\u001b[0m\n",
      "\u001b[32m2025-06-20 05:29:19.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m695\u001b[0m - \u001b[1mCompiling function...\u001b[0m\n",
      "\u001b[32m2025-06-20 05:29:19.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mLoading VQ-GAN model...\u001b[0m\n",
      "/home/ubuntu/miniconda3/envs/fish-speech/lib/python3.11/site-packages/vector_quantize_pytorch/vector_quantize_pytorch.py:445: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/ubuntu/miniconda3/envs/fish-speech/lib/python3.11/site-packages/vector_quantize_pytorch/vector_quantize_pytorch.py:630: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/ubuntu/miniconda3/envs/fish-speech/lib/python3.11/site-packages/vector_quantize_pytorch/finite_scalar_quantization.py:147: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/ubuntu/miniconda3/envs/fish-speech/lib/python3.11/site-packages/vector_quantize_pytorch/lookup_free_quantization.py:209: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "\u001b[32m2025-06-20 05:29:22.001\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.vqgan.inference\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m46\u001b[0m - \u001b[1mLoaded model: <All keys matched successfully>\u001b[0m\n",
      "\u001b[32m2025-06-20 05:29:22.001\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m71\u001b[0m - \u001b[1mDecoder model loaded, warming up...\u001b[0m\n",
      "\u001b[32m2025-06-20 05:29:22.013\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Hello world.\u001b[0m\n",
      "\u001b[32m2025-06-20 05:29:22.013\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/1 of sample 1/1\u001b[0m\n",
      "  0%|                                                  | 0/1023 [00:00<?, ?it/s]/home/ubuntu/miniconda3/envs/fish-speech/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/home/ubuntu/miniconda3/envs/fish-speech/lib/python3.11/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "  0%|                                      | 1/1023 [03:13<55:02:10, 193.87s/it]/home/ubuntu/miniconda3/envs/fish-speech/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "  0%|                                      | 1/1023 [03:13<55:03:24, 193.94s/it]\n",
      "\u001b[32m2025-06-20 05:32:38.398\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m98\u001b[0m - \u001b[1mWarming up done, launching the web UI...\u001b[0m\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n",
      "\u001b[32m2025-06-20 05:34:28.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: നോബ്രോക്കർ-ൽ വീട് കണ്ടെത്തുന്നത് വളരെ എളുപ്പമാണ്,\u001b[0m\n",
      "\u001b[32m2025-06-20 05:34:28.261\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: കാരണം ഇവിടെ ബ്രോക്കറേജ് ഒന്നും നൽകേണ്ടതില്ല.\u001b[0m\n",
      "\u001b[32m2025-06-20 05:34:28.261\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/2 of sample 1/1\u001b[0m\n",
      "  0%|                                                  | 0/8039 [00:00<?, ?it/s]/home/ubuntu/miniconda3/envs/fish-speech/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "  0%|                                        | 1/8039 [00:01<2:24:24,  1.08s/it]/home/ubuntu/miniconda3/envs/fish-speech/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "  0%|                                        | 1/8039 [00:01<2:24:42,  1.08s/it]\n",
      "\u001b[32m2025-06-20 05:34:37.682\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: നോബ്രോക്കർ-ൽ വീട് കണ്ടെത്തുന്നത് വളരെ എളുപ്പമാണ്,\u001b[0m\n",
      "\u001b[32m2025-06-20 05:34:37.689\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: കാരണം ഇവിടെ ബ്രോക്കറേജ് ഒന്നും നൽകേണ്ടതില്ല.\u001b[0m\n",
      "\u001b[32m2025-06-20 05:34:37.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/2 of sample 1/1\u001b[0m\n",
      "  1%|▌                                      | 109/8039 [00:00<00:40, 197.52it/s]\n",
      "\u001b[32m2025-06-20 05:34:38.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 0.64 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 05:34:38.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 111 tokens in 0.64 seconds, 173.34 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 05:34:38.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 110.58 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 05:34:38.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 1.81 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 05:34:38.362\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/2 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 05:34:38.362\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 110])\u001b[0m\n",
      "  1%|▌                                       | 98/7798 [00:00<00:38, 198.65it/s]\n",
      "\u001b[32m2025-06-20 05:34:39.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 100 tokens in 0.76 seconds, 131.99 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 05:34:39.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 84.20 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 05:34:39.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 1.82 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 05:34:39.620\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 99])\u001b[0m\n",
      "/home/ubuntu/miniconda3/envs/fish-speech/lib/python3.11/site-packages/gradio/processing_utils.py:753: UserWarning: Trying to convert audio automatically from float32 to 16-bit int format.\n",
      "  warnings.warn(warning.format(data.dtype))\n",
      "\u001b[32m2025-06-20 05:35:27.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: नोब्रोकर से घर ढूंढना इतना आसान है,\u001b[0m\n",
      "\u001b[32m2025-06-20 05:35:27.387\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: क्योंकि यहाँ कोई ब्रोकरेज नहीं देनी पड़ती।\u001b[0m\n",
      "\u001b[32m2025-06-20 05:35:27.388\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/2 of sample 1/1\u001b[0m\n",
      "  3%|█▎                                     | 283/8102 [00:01<00:41, 188.53it/s]\n",
      "\u001b[32m2025-06-20 05:35:29.011\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 1.62 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 05:35:29.011\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 285 tokens in 1.62 seconds, 175.62 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 05:35:29.011\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 112.03 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 05:35:29.011\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 1.92 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 05:35:29.012\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/2 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 05:35:29.013\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 284])\u001b[0m\n",
      "  3%|█                                      | 217/7719 [00:01<00:37, 198.11it/s]\n",
      "\u001b[32m2025-06-20 05:35:30.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 219 tokens in 1.26 seconds, 173.49 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 05:35:30.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 110.67 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 05:35:30.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.18 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 05:35:30.276\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 218])\u001b[0m\n",
      "\u001b[32m2025-06-20 05:36:01.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: नोब्रोकर से घर ढूंढना इतना आसान है,\u001b[0m\n",
      "\u001b[32m2025-06-20 05:36:01.372\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: क्योंकि यहाँ कोई ब्रोकरेज नहीं देनी पड़ती।\u001b[0m\n",
      "\u001b[32m2025-06-20 05:36:01.372\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/2 of sample 1/1\u001b[0m\n",
      "  1%|▍                                       | 81/8102 [00:00<00:42, 189.78it/s]\n",
      "\u001b[32m2025-06-20 05:36:01.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 0.54 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 05:36:01.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 83 tokens in 0.54 seconds, 153.16 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 05:36:01.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 97.71 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 05:36:01.915\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.18 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 05:36:01.915\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/2 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 05:36:01.916\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 82])\u001b[0m\n",
      "  1%|▎                                       | 63/7921 [00:00<00:38, 201.66it/s]\n",
      "\u001b[32m2025-06-20 05:36:02.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 65 tokens in 0.45 seconds, 144.63 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 05:36:02.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 92.26 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 05:36:02.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.18 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 05:36:02.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 64])\u001b[0m\n",
      "\u001b[32m2025-06-20 05:36:26.372\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: नोब्रोकर से घर ढूंढना इतना आसान है, क्योंकि यहाँ कोई ब्रोकरेज नहीं देनी पड़ती।\n",
      "\u001b[0m\n",
      "\u001b[32m2025-06-20 05:36:26.372\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/1 of sample 1/1\u001b[0m\n",
      "  2%|▊                                      | 158/8012 [00:00<00:39, 200.63it/s]\n",
      "\u001b[32m2025-06-20 05:36:27.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 0.88 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 05:36:27.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 160 tokens in 0.88 seconds, 182.37 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 05:36:27.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 116.34 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 05:36:27.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.18 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 05:36:27.251\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 159])\u001b[0m\n",
      "\u001b[32m2025-06-20 05:36:43.665\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: नोब्रोकर से घर ढूंढना इतना आसान है, क्योंकि यहाँ कोई ब्रोकरेज नहीं देनी पड़ती।\u001b[0m\n",
      "\u001b[32m2025-06-20 05:36:43.666\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/1 of sample 1/1\u001b[0m\n",
      "  2%|▊                                      | 168/8012 [00:00<00:42, 184.48it/s]\n",
      "\u001b[32m2025-06-20 05:36:44.675\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 1.01 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 05:36:44.675\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 170 tokens in 1.01 seconds, 168.42 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 05:36:44.675\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 107.44 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 05:36:44.676\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.18 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 05:36:44.676\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 169])\u001b[0m\n",
      "\u001b[32m2025-06-20 05:37:14.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: नोब्रोकर से घर ढूंढना इतना आसान है,\u001b[0m\n",
      "\u001b[32m2025-06-20 05:37:14.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: क्योंकि यहाँ कोई ब्रोकरेज नहीं देनी पड़ती।\u001b[0m\n",
      "\u001b[32m2025-06-20 05:37:14.461\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/2 of sample 1/1\u001b[0m\n",
      "  1%|▍                                       | 91/8102 [00:00<00:42, 190.32it/s]\n",
      "\u001b[32m2025-06-20 05:37:15.066\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 0.61 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 05:37:15.066\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 93 tokens in 0.61 seconds, 153.55 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 05:37:15.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 97.95 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 05:37:15.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.18 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 05:37:15.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/2 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 05:37:15.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 92])\u001b[0m\n",
      "  1%|▍                                       | 76/7911 [00:00<00:41, 188.57it/s]\n",
      "\u001b[32m2025-06-20 05:37:15.605\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 78 tokens in 0.54 seconds, 145.05 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 05:37:15.605\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 92.53 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 05:37:15.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.18 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 05:37:15.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 77])\u001b[0m\n",
      "\u001b[32m2025-06-20 05:37:52.443\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: नोब्रोकर से घर ढूंढना इतना आसान है,\u001b[0m\n",
      "\u001b[32m2025-06-20 05:37:52.451\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: क्योंकि यहाँ कोई ब्रोकरेज नहीं देनी पड़ती।\u001b[0m\n",
      "\u001b[32m2025-06-20 05:37:52.451\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/2 of sample 1/1\u001b[0m\n",
      "  1%|▍                                       | 81/8102 [00:00<00:42, 187.27it/s]\n",
      "\u001b[32m2025-06-20 05:37:52.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 0.53 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 05:37:52.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 83 tokens in 0.53 seconds, 156.45 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 05:37:52.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 99.80 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 05:37:52.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.18 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 05:37:52.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/2 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 05:37:52.983\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 82])\u001b[0m\n",
      "  1%|▍                                       | 77/7921 [00:00<00:40, 194.63it/s]\n",
      "\u001b[32m2025-06-20 05:37:53.496\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 79 tokens in 0.51 seconds, 154.16 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 05:37:53.496\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 98.34 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 05:37:53.496\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.18 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 05:37:53.497\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 78])\u001b[0m\n",
      "\u001b[32m2025-06-20 05:38:28.867\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mLoaded audio with 2.60 seconds\u001b[0m\n",
      "/home/ubuntu/miniconda3/envs/fish-speech/lib/python3.11/site-packages/vector_quantize_pytorch/residual_fsq.py:170: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled = False):\n",
      "\u001b[32m2025-06-20 05:38:29.670\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mEncoded prompt: torch.Size([8, 56])\u001b[0m\n",
      "\u001b[32m2025-06-20 05:38:29.694\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: नोब्रोकर से घर ढूंढना इतना आसान है,\u001b[0m\n",
      "\u001b[32m2025-06-20 05:38:29.702\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: क्योंकि यहाँ कोई ब्रोकरेज नहीं देनी पड़ती।\u001b[0m\n",
      "\u001b[32m2025-06-20 05:38:29.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/2 of sample 1/1\u001b[0m\n",
      "  1%|▌                                      | 118/8023 [00:00<00:42, 185.82it/s]\n",
      "\u001b[32m2025-06-20 05:38:30.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 0.75 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 05:38:30.454\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 120 tokens in 0.75 seconds, 159.83 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 05:38:30.454\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 101.96 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 05:38:30.454\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.18 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 05:38:30.455\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/2 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 05:38:30.455\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 119])\u001b[0m\n",
      "  1%|▎                                       | 69/7805 [00:00<00:39, 195.54it/s]\n",
      "\u001b[32m2025-06-20 05:38:30.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 71 tokens in 0.51 seconds, 139.83 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 05:38:30.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 89.20 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 05:38:30.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.18 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 05:38:30.965\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 70])\u001b[0m\n",
      "\u001b[32m2025-06-20 05:39:03.228\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mLoaded audio with 2.13 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 05:39:03.266\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mEncoded prompt: torch.Size([8, 46])\u001b[0m\n",
      "\u001b[32m2025-06-20 05:39:03.293\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: नोब्रोकर से घर ढूंढना इतना आसान है,\u001b[0m\n",
      "\u001b[32m2025-06-20 05:39:03.305\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: क्योंकि यहाँ कोई ब्रोकरेज नहीं देनी पड़ती।\u001b[0m\n",
      "\u001b[32m2025-06-20 05:39:03.306\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/2 of sample 1/1\u001b[0m\n",
      "  1%|▍                                       | 79/8033 [00:00<00:45, 175.70it/s]\n",
      "\u001b[32m2025-06-20 05:39:03.868\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 0.56 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 05:39:03.868\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 81 tokens in 0.56 seconds, 144.05 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 05:39:03.868\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 91.89 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 05:39:03.869\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.18 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 05:39:03.869\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/2 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 05:39:03.871\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 80])\u001b[0m\n",
      "  1%|▎                                       | 62/7854 [00:00<00:40, 190.14it/s]\n",
      "\u001b[32m2025-06-20 05:39:04.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 64 tokens in 0.49 seconds, 129.50 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 05:39:04.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 82.61 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 05:39:04.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.18 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 05:39:04.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 63])\u001b[0m\n",
      "\u001b[32m2025-06-20 05:42:13.167\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.reference_loader\u001b[0m:\u001b[36mload_by_hash\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mUse same references\u001b[0m\n",
      "\u001b[32m2025-06-20 05:42:13.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: नोब्रोकर से घर ढूंढना इतना आसान है,\u001b[0m\n",
      "\u001b[32m2025-06-20 05:42:13.198\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: क्योंकि यहाँ कोई ब्रोकरेज नहीं देनी पड़ती।\u001b[0m\n",
      "\u001b[32m2025-06-20 05:42:13.198\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/2 of sample 1/1\u001b[0m\n",
      "  1%|▍                                       | 84/8033 [00:00<00:41, 192.12it/s]\n",
      "\u001b[32m2025-06-20 05:42:13.729\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 0.53 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 05:42:13.729\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 86 tokens in 0.53 seconds, 162.01 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 05:42:13.729\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 103.35 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 05:42:13.729\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.18 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 05:42:13.730\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/2 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 05:42:13.731\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 85])\u001b[0m\n",
      "  1%|▎                                       | 56/7849 [00:00<00:38, 201.37it/s]\n",
      "\u001b[32m2025-06-20 05:42:14.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 58 tokens in 0.40 seconds, 144.52 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 05:42:14.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 92.19 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 05:42:14.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.18 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 05:42:14.133\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 57])\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:14.391\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.reference_loader\u001b[0m:\u001b[36mload_by_hash\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mUse same references\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:14.413\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: नोब्रोकर से घर ढूंढना इतना आसान और सुविधाजनक है, क्योंकि यहाँ आपको कोई\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:14.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: ब्रोकरेज फीस नहीं देनी पड़ती और आप सीधे मकान मालिक से बात कर सकते हैं,\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:14.428\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: जिससे आपका समय और पैसा दोनों बचता है।\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:14.428\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/3 of sample 1/1\u001b[0m\n",
      "  1%|▍                                       | 99/7964 [00:00<00:43, 180.75it/s]\n",
      "\u001b[32m2025-06-20 05:43:15.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 0.64 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:15.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 101 tokens in 0.64 seconds, 156.93 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:15.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 100.11 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:15.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.18 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:15.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/3 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:15.074\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 100])\u001b[0m\n",
      "  1%|▌                                      | 109/7716 [00:00<00:38, 199.78it/s]\n",
      "\u001b[32m2025-06-20 05:43:15.772\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 111 tokens in 0.70 seconds, 158.83 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:15.772\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 101.32 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:15.772\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.18 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:15.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/3 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:15.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 110])\u001b[0m\n",
      "  1%|▎                                       | 62/7523 [00:00<00:37, 200.67it/s]\n",
      "\u001b[32m2025-06-20 05:43:16.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 64 tokens in 0.42 seconds, 153.13 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:16.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 97.68 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:16.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.18 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:16.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 63])\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:44.264\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.reference_loader\u001b[0m:\u001b[36mload_by_hash\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mUse same references\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:44.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: नोब्रोकर से घर ढूंढना इतना आसान और सुविधाजनक है, क्योंकि यहाँ आपको कोई\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:44.295\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: ब्रोकरेज फीस नहीं देनी पड़ती और आप सीधे मकान मालिक से बात कर सकते हैं,\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:44.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: जिससे आपका समय और पैसा दोनों बचता है।\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:44.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/3 of sample 1/1\u001b[0m\n",
      "  1%|▍                                       | 98/7964 [00:00<00:41, 191.13it/s]\n",
      "\u001b[32m2025-06-20 05:43:44.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 0.60 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:44.904\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 100 tokens in 0.60 seconds, 166.62 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:44.904\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 106.29 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:44.904\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.18 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:44.904\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/3 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:44.905\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 99])\u001b[0m\n",
      "  1%|▌                                      | 107/7717 [00:00<00:37, 204.13it/s]\n",
      "\u001b[32m2025-06-20 05:43:45.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 109 tokens in 0.64 seconds, 171.58 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:45.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 109.46 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:45.541\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.18 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:45.541\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/3 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:45.542\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 108])\u001b[0m\n",
      "  1%|▍                                       | 84/7526 [00:00<00:37, 200.72it/s]\n",
      "\u001b[32m2025-06-20 05:43:46.088\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 86 tokens in 0.55 seconds, 157.21 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:46.088\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 100.29 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:46.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.18 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 05:43:46.090\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 85])\u001b[0m\n",
      "\u001b[32m2025-06-20 05:46:19.983\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.reference_loader\u001b[0m:\u001b[36mload_by_hash\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mUse same references\u001b[0m\n",
      "\u001b[32m2025-06-20 05:46:20.012\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: നോബ്രോക്കർ-ൽ വീട് കണ്ടെത്തുന്നത് വളരെ എളുപ്പവും സൗകര്യപ്രദവുമാണ്,\u001b[0m\n",
      "\u001b[32m2025-06-20 05:46:20.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: കാരണം ഇവിടെ ബ്രോക്കറേജ് ഫീസ് ഒന്നും നൽകേണ്ടതില്ല,\u001b[0m\n",
      "\u001b[32m2025-06-20 05:46:20.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: കൂടാതെ നിങ്ങൾക്ക് നേരിട്ട് വീട്ടുടമയുമായി ബന്ധപ്പെടാൻ കഴിയും,\u001b[0m\n",
      "\u001b[32m2025-06-20 05:46:20.040\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: ഇത് നിങ്ങളുടെ സമയവും പണവും ലാഭിക്കാൻ സഹായിക്കുന്നു.\u001b[0m\n",
      "\u001b[32m2025-06-20 05:46:20.040\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/4 of sample 1/1\u001b[0m\n",
      "  1%|▍                                       | 86/7924 [00:00<00:43, 181.80it/s]\n",
      "\u001b[32m2025-06-20 05:46:20.626\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 0.59 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 05:46:20.626\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 88 tokens in 0.59 seconds, 150.33 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 05:46:20.626\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 95.90 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 05:46:20.626\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.18 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 05:46:20.627\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/4 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 05:46:20.628\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 87])\u001b[0m\n",
      "  1%|▍                                       | 92/7693 [00:00<00:39, 194.89it/s]\n",
      "\u001b[32m2025-06-20 05:46:21.261\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 94 tokens in 0.63 seconds, 148.32 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 05:46:21.261\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 94.62 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 05:46:21.262\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.18 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 05:46:21.262\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/4 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 05:46:21.263\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 93])\u001b[0m\n",
      "  1%|▌                                      | 107/7420 [00:00<00:38, 191.65it/s]\n",
      "\u001b[32m2025-06-20 05:46:21.997\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 109 tokens in 0.73 seconds, 148.41 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 05:46:21.997\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 94.67 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 05:46:21.998\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.18 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 05:46:21.998\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 4/4 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 05:46:21.999\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 108])\u001b[0m\n",
      "  1%|▍                                       | 82/7162 [00:00<00:35, 198.57it/s]\n",
      "\u001b[32m2025-06-20 05:46:22.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 84 tokens in 0.54 seconds, 155.21 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 05:46:22.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 99.01 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 05:46:22.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.18 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 05:46:22.541\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 83])\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:32.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mLoaded audio with 365.12 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:32.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mEncoded prompt: torch.Size([8, 7862])\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:33.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: നോബ്രോക്കർ-ൽ വീട് കണ്ടെത്തുന്നത് വളരെ എളുപ്പവും സൗകര്യപ്രദവുമാണ്,\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:33.126\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: കാരണം ഇവിടെ ബ്രോക്കറേജ് ഫീസ് ഒന്നും നൽകേണ്ടതില്ല,\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:33.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: കൂടാതെ നിങ്ങൾക്ക് നേരിട്ട് വീട്ടുടമയുമായി ബന്ധപ്പെടാൻ കഴിയും,\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:33.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: ഇത് നിങ്ങളുടെ സമയവും പണവും ലാഭിക്കാൻ സഹായിക്കുന്നു.\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:33.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/4 of sample 1/1\u001b[0m\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 202.02it/s]\n",
      "\u001b[32m2025-06-20 06:25:33.892\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 0.75 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:33.892\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 98 tokens in 0.75 seconds, 130.08 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:33.892\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 82.98 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:33.893\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.68 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:33.893\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/4 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:33.894\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 97])\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:47.308\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mLoaded audio with 5.19 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:47.361\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mEncoded prompt: torch.Size([8, 112])\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:47.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: നോബ്രോക്കർ-ൽ വീട് കണ്ടെത്തുന്നത് വളരെ എളുപ്പവും സൗകര്യപ്രദവുമാണ്,\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:47.384\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: കാരണം ഇവിടെ ബ്രോക്കറേജ് ഫീസ് ഒന്നും നൽകേണ്ടതില്ല,\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:47.390\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: കൂടാതെ നിങ്ങൾക്ക് നേരിട്ട് വീട്ടുടമയുമായി ബന്ധപ്പെടാൻ കഴിയും,\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:47.395\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: ഇത് നിങ്ങളുടെ സമയവും പണവും ലാഭിക്കാൻ സഹായിക്കുന്നു.\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:47.395\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/4 of sample 1/1\u001b[0m\n",
      "  2%|▋                                      | 142/7849 [00:00<00:39, 194.29it/s]\n",
      "\u001b[32m2025-06-20 06:25:48.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 0.83 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:48.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 144 tokens in 0.83 seconds, 174.27 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:48.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 111.17 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:48.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:48.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/4 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:48.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 143])\u001b[0m\n",
      "  1%|▍                                       | 74/7562 [00:00<00:37, 197.70it/s]\n",
      "\u001b[32m2025-06-20 06:25:48.730\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 76 tokens in 0.51 seconds, 149.91 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:48.730\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 95.63 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:48.731\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:48.731\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/4 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:48.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 75])\u001b[0m\n",
      "  1%|▌                                       | 97/7307 [00:00<00:36, 199.98it/s]\n",
      "\u001b[32m2025-06-20 06:25:49.346\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 99 tokens in 0.61 seconds, 161.22 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:49.346\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 102.84 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:49.346\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:49.347\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 4/4 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:49.348\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 98])\u001b[0m\n",
      "  1%|▍                                       | 68/7059 [00:00<00:35, 194.25it/s]\n",
      "\u001b[32m2025-06-20 06:25:49.826\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 70 tokens in 0.48 seconds, 146.08 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:49.826\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 93.19 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:49.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:25:49.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 69])\u001b[0m\n",
      "\u001b[32m2025-06-20 06:26:21.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.reference_loader\u001b[0m:\u001b[36mload_by_hash\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mUse same references\u001b[0m\n",
      "\u001b[32m2025-06-20 06:26:21.652\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: नोब्रोकर से घर ढूंढना इतना आसान और सुविधाजनक है, क्योंकि यहाँ आपको कोई\u001b[0m\n",
      "\u001b[32m2025-06-20 06:26:21.658\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: ब्रोकरेज फीस नहीं देनी पड़ती और आप सीधे मकान मालिक से बात कर सकते हैं,\u001b[0m\n",
      "\u001b[32m2025-06-20 06:26:21.664\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: जिससे आपका समय और पैसा दोनों बचता है।\u001b[0m\n",
      "\u001b[32m2025-06-20 06:26:21.664\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/3 of sample 1/1\u001b[0m\n",
      "  2%|▋                                      | 147/7889 [00:00<00:39, 194.43it/s]\n",
      "\u001b[32m2025-06-20 06:26:22.520\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 0.86 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 06:26:22.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 149 tokens in 0.86 seconds, 174.03 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:26:22.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 111.02 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:26:22.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:26:22.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/3 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:26:22.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 148])\u001b[0m\n",
      "  2%|▌                                      | 116/7593 [00:00<00:37, 198.99it/s]\n",
      "\u001b[32m2025-06-20 06:26:23.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 118 tokens in 0.71 seconds, 166.31 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:26:23.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 106.09 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:26:23.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:26:23.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/3 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:26:23.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 117])\u001b[0m\n",
      "  1%|▍                                       | 76/7393 [00:00<00:36, 199.09it/s]\n",
      "\u001b[32m2025-06-20 06:26:23.739\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 78 tokens in 0.51 seconds, 153.95 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:26:23.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 98.21 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:26:23.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:26:23.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 77])\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:42.594\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mLoaded audio with 4.76 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:42.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mEncoded prompt: torch.Size([8, 103])\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:42.657\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: नोब्रोकर से घर ढूंढना इतना आसान और सुविधाजनक है, क्योंकि यहाँ आपको कोई\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:42.663\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: ब्रोकरेज फीस नहीं देनी पड़ती और आप सीधे मकान मालिक से बात कर सकते हैं,\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:42.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: जिससे आपका समय और पैसा दोनों बचता है।\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:42.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/3 of sample 1/1\u001b[0m\n",
      "  2%|▋                                      | 129/7898 [00:00<00:39, 197.47it/s]\n",
      "\u001b[32m2025-06-20 06:27:43.411\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 0.74 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:43.411\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 131 tokens in 0.74 seconds, 176.43 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:43.411\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 112.55 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:43.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:43.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/3 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:43.413\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 130])\u001b[0m\n",
      "  1%|▌                                      | 109/7620 [00:00<00:37, 200.84it/s]\n",
      "\u001b[32m2025-06-20 06:27:44.083\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 111 tokens in 0.67 seconds, 165.49 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:44.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 105.57 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:44.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:44.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/3 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:44.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 110])\u001b[0m\n",
      "  1%|▍                                       | 82/7427 [00:00<00:37, 193.49it/s]\n",
      "\u001b[32m2025-06-20 06:27:44.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 84 tokens in 0.53 seconds, 157.78 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:44.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 100.65 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:44.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:44.618\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 83])\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:56.328\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.reference_loader\u001b[0m:\u001b[36mload_by_hash\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mUse same references\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:56.347\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: नोब्रोकर से घर ढूंढना इतना आसान और सुविधाजनक है, क्योंकि यहाँ आपको कोई\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:56.353\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: ब्रोकरेज फीस नहीं देनी पड़ती और आप सीधे मकान मालिक से बात कर सकते हैं,\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:56.358\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: जिससे आपका समय और पैसा दोनों बचता है।\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:56.358\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/3 of sample 1/1\u001b[0m\n",
      " 38%|███████████████▏                        | 194/511 [00:01<00:01, 194.00it/s]\n",
      "\u001b[32m2025-06-20 06:27:57.450\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 1.09 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:57.450\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 196 tokens in 1.09 seconds, 179.53 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:57.450\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 114.52 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:57.451\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:57.451\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/3 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:57.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 195])\u001b[0m\n",
      " 21%|████████▍                               | 107/511 [00:00<00:02, 199.47it/s]\n",
      "\u001b[32m2025-06-20 06:27:58.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 109 tokens in 0.68 seconds, 161.40 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:58.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 102.96 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:58.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:58.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/3 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:58.129\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 108])\u001b[0m\n",
      " 16%|██████▋                                  | 83/511 [00:00<00:02, 200.15it/s]\n",
      "\u001b[32m2025-06-20 06:27:58.662\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 85 tokens in 0.53 seconds, 159.27 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:58.662\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 101.60 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:58.662\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:27:58.663\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 84])\u001b[0m\n",
      "\u001b[32m2025-06-20 06:28:34.492\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.reference_loader\u001b[0m:\u001b[36mload_by_hash\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mUse same references\u001b[0m\n",
      "\u001b[32m2025-06-20 06:28:34.512\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: नोब्रोकर से घर ढूंढना इतना आसान और सुविधाजनक है, क्योंकि यहाँ आपको कोई\u001b[0m\n",
      "\u001b[32m2025-06-20 06:28:34.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: ब्रोकरेज फीस नहीं देनी पड़ती और आप सीधे मकान मालिक से बात कर सकते हैं,\u001b[0m\n",
      "\u001b[32m2025-06-20 06:28:34.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: जिससे आपका समय और पैसा दोनों बचता है।\u001b[0m\n",
      "\u001b[32m2025-06-20 06:28:34.523\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/3 of sample 1/1\u001b[0m\n",
      " 20%|███████▉                                | 102/511 [00:00<00:02, 187.29it/s]\n",
      "\u001b[32m2025-06-20 06:28:35.155\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 0.63 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 06:28:35.155\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 104 tokens in 0.63 seconds, 164.48 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:28:35.155\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 104.93 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:28:35.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:28:35.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/3 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:28:35.157\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 103])\u001b[0m\n",
      " 31%|████████████▍                           | 159/511 [00:00<00:01, 198.68it/s]\n",
      "\u001b[32m2025-06-20 06:28:36.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 161 tokens in 0.94 seconds, 171.26 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:28:36.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 109.25 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:28:36.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:28:36.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/3 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:28:36.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 160])\u001b[0m\n",
      " 13%|█████▍                                   | 68/511 [00:00<00:02, 199.86it/s]\n",
      "\u001b[32m2025-06-20 06:28:36.561\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 70 tokens in 0.46 seconds, 151.21 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:28:36.561\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 96.46 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:28:36.561\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:28:36.562\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 69])\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.348\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.reference_loader\u001b[0m:\u001b[36mload_by_hash\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mUse same references\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: नोब्\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.367\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: रोकर\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.371\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: से\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.376\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: घर\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: ढूंढ\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.384\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: ना\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.387\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: इतना\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.390\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: आसान\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.393\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: और\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.397\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: सुवि\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.400\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: धाजन\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.404\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: क\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: है,\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: क्यो\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.416\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: ंकि\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: यहाँ\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: आपको\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.426\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: कोई\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.432\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: ब्रो\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.436\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: करेज\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.440\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: फीस\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.445\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: नहीं\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.449\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: देनी\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: पड़त\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.455\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: ी\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.459\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: और\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: आप\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: सीधे\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.470\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: मकान\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.474\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: मालि\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.478\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: क\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: से\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: बात\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.488\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: कर\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: सकते\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.495\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: हैं,\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.499\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: जिसस\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.503\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: े\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.507\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: आपका\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: समय\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.513\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: और\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: पैसा\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: दोनो\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.525\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: ं\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.530\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: बचता\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: है।\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/46 of sample 1/1\u001b[0m\n",
      "  0%|                                        | 21/8026 [00:00<00:52, 152.05it/s]\n",
      "\u001b[32m2025-06-20 06:29:15.762\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 0.23 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.762\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 23 tokens in 0.23 seconds, 101.16 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.762\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 64.53 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.763\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.763\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.764\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 22])\u001b[0m\n",
      "  0%|                                        | 14/7985 [00:00<00:45, 173.62it/s]\n",
      "\u001b[32m2025-06-20 06:29:15.978\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 16 tokens in 0.21 seconds, 74.54 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.978\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 47.55 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.979\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.979\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:15.984\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 15])\u001b[0m\n",
      "  0%|                                        | 24/7955 [00:00<00:43, 184.31it/s]\n",
      "\u001b[32m2025-06-20 06:29:16.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 26 tokens in 0.26 seconds, 101.83 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:16.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 64.96 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:16.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:16.236\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 4/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:16.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 25])\u001b[0m\n",
      "  0%|                                        | 11/7916 [00:00<00:45, 173.86it/s]\n",
      "\u001b[32m2025-06-20 06:29:16.425\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 13 tokens in 0.19 seconds, 68.83 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:16.425\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 43.91 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:16.426\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:16.426\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 5/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:16.428\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 12])\u001b[0m\n",
      "  0%|                                        | 15/7885 [00:00<00:42, 184.63it/s]\n",
      "\u001b[32m2025-06-20 06:29:16.634\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 17 tokens in 0.21 seconds, 81.88 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:16.634\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 52.23 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:16.634\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:16.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 6/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:16.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 16])\u001b[0m\n",
      "  0%|                                        | 11/7855 [00:00<00:44, 174.77it/s]\n",
      "\u001b[32m2025-06-20 06:29:16.822\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 13 tokens in 0.19 seconds, 69.61 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:16.822\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 44.40 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:16.822\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:16.823\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 7/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:16.824\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 12])\u001b[0m\n",
      "  0%|                                        | 14/7825 [00:00<00:42, 183.29it/s]\n",
      "\u001b[32m2025-06-20 06:29:17.011\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 16 tokens in 0.19 seconds, 85.08 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:17.011\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 54.27 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:17.012\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:17.012\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 8/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:17.013\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 15])\u001b[0m\n",
      "  0%|                                        | 13/7792 [00:00<00:42, 184.00it/s]\n",
      "\u001b[32m2025-06-20 06:29:17.193\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 15 tokens in 0.18 seconds, 83.11 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:17.193\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 53.02 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:17.193\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:17.194\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 9/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:17.195\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 14])\u001b[0m\n",
      "  0%|                                        | 15/7764 [00:00<00:42, 181.18it/s]\n",
      "\u001b[32m2025-06-20 06:29:17.406\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 17 tokens in 0.21 seconds, 80.19 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:17.406\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 51.15 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:17.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:17.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 10/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:17.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 16])\u001b[0m\n",
      "  0%|                                        | 20/7729 [00:00<00:40, 190.25it/s]\n",
      "\u001b[32m2025-06-20 06:29:17.621\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 22 tokens in 0.21 seconds, 102.99 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:17.621\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 65.70 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:17.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:17.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 11/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:17.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 21])\u001b[0m\n",
      "  0%|                                        | 13/7690 [00:00<00:42, 180.82it/s]\n",
      "\u001b[32m2025-06-20 06:29:17.833\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 15 tokens in 0.21 seconds, 71.16 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:17.833\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 45.39 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:17.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:17.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 12/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:17.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 14])\u001b[0m\n",
      "  0%|                                        | 12/7664 [00:00<00:44, 172.04it/s]\n",
      "\u001b[32m2025-06-20 06:29:18.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 14 tokens in 0.20 seconds, 69.72 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:18.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 44.48 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:18.036\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:18.036\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 13/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:18.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 13])\u001b[0m\n",
      "  0%|                                         | 5/7635 [00:00<00:54, 139.39it/s]\n",
      "\u001b[32m2025-06-20 06:29:18.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 7 tokens in 0.20 seconds, 34.97 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:18.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 22.31 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:18.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:18.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 14/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:18.300\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 6])\u001b[0m\n",
      "  0%|                                        | 10/7609 [00:00<00:43, 174.91it/s]\n",
      "\u001b[32m2025-06-20 06:29:18.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 12 tokens in 0.24 seconds, 50.31 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:18.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 32.09 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:18.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:18.478\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 15/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:18.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 11])\u001b[0m\n",
      "  0%|                                        | 20/7582 [00:00<00:42, 176.07it/s]\n",
      "\u001b[32m2025-06-20 06:29:18.729\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 22 tokens in 0.25 seconds, 87.54 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:18.729\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 55.84 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:18.730\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:18.730\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 16/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:18.745\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 21])\u001b[0m\n",
      "  0%|                                        | 23/7543 [00:00<00:39, 189.00it/s]\n",
      "\u001b[32m2025-06-20 06:29:18.974\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 25 tokens in 0.24 seconds, 102.81 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:18.974\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 65.59 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:18.974\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:18.975\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 17/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:18.975\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 24])\u001b[0m\n",
      "  0%|                                        | 18/7500 [00:00<00:40, 184.13it/s]\n",
      "\u001b[32m2025-06-20 06:29:19.224\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 20 tokens in 0.25 seconds, 80.63 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:19.224\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 51.44 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:19.224\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:19.224\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 18/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:19.225\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 19])\u001b[0m\n",
      "  0%|                                        | 15/7464 [00:00<00:40, 184.24it/s]\n",
      "\u001b[32m2025-06-20 06:29:19.437\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 17 tokens in 0.21 seconds, 80.01 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:19.437\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 51.04 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:19.438\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:19.438\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 19/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:19.440\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 16])\u001b[0m\n",
      "  0%|                                        | 10/7428 [00:00<00:41, 176.75it/s]\n",
      "\u001b[32m2025-06-20 06:29:19.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 12 tokens in 0.18 seconds, 67.87 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:19.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 43.30 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:19.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:19.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 20/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:19.618\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 11])\u001b[0m\n",
      "  0%|                                        | 16/7398 [00:00<00:40, 183.14it/s]\n",
      "\u001b[32m2025-06-20 06:29:19.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 18 tokens in 0.21 seconds, 85.73 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:19.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 54.69 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:19.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:19.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 21/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:19.830\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 17])\u001b[0m\n",
      "  0%|                                        | 21/7364 [00:00<00:38, 192.43it/s]\n",
      "\u001b[32m2025-06-20 06:29:20.062\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 23 tokens in 0.23 seconds, 98.87 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:20.062\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 63.07 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:20.062\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:20.062\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 22/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:20.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 22])\u001b[0m\n",
      "  0%|                                        | 17/7323 [00:00<00:38, 189.20it/s]\n",
      "\u001b[32m2025-06-20 06:29:20.261\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 19 tokens in 0.20 seconds, 96.02 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:20.261\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 61.26 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:20.261\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:20.261\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 23/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:20.262\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 18])\u001b[0m\n",
      "  0%|                                        | 18/7285 [00:00<00:38, 190.50it/s]\n",
      "\u001b[32m2025-06-20 06:29:20.478\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 20 tokens in 0.22 seconds, 92.53 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:20.478\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 59.03 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:20.478\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:20.479\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 24/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:20.480\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 19])\u001b[0m\n",
      "  0%|                                        | 12/7248 [00:00<00:39, 185.17it/s]\n",
      "\u001b[32m2025-06-20 06:29:20.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 14 tokens in 0.17 seconds, 81.55 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:20.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 52.02 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:20.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:20.652\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 25/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:20.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 13])\u001b[0m\n",
      "  0%|                                        | 10/7222 [00:00<00:39, 182.29it/s]\n",
      "\u001b[32m2025-06-20 06:29:20.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 12 tokens in 0.16 seconds, 73.27 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:20.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 46.74 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:20.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:20.817\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 26/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:20.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 11])\u001b[0m\n",
      "  3%|█▎                                     | 245/7197 [00:01<00:35, 195.58it/s]\n",
      "\u001b[32m2025-06-20 06:29:22.178\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 247 tokens in 1.36 seconds, 181.54 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:22.178\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 115.81 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:22.178\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:22.178\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 27/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:22.180\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 246])\u001b[0m\n",
      "  2%|▌                                      | 105/6937 [00:00<00:36, 186.74it/s]\n",
      "\u001b[32m2025-06-20 06:29:22.875\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 107 tokens in 0.70 seconds, 153.69 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:22.875\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 98.04 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:22.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:22.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 28/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:22.878\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 106])\u001b[0m\n",
      " 15%|█████▋                                | 1026/6811 [00:05<00:30, 190.54it/s]\n",
      "\u001b[32m2025-06-20 06:29:28.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 1028 tokens in 5.54 seconds, 185.45 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:28.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 118.30 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:28.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.69 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:28.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 29/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:28.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 1027])\u001b[0m\n",
      "  3%|█                                      | 153/5766 [00:00<00:30, 186.61it/s]\n",
      "\u001b[32m2025-06-20 06:29:29.441\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 155 tokens in 1.02 seconds, 152.02 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:29.441\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 96.98 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:29.442\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 3.33 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:29.442\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 30/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:29.443\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 154])\u001b[0m\n",
      "  2%|▊                                      | 109/5594 [00:00<00:30, 181.76it/s]\n",
      "\u001b[32m2025-06-20 06:29:30.245\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 111 tokens in 0.80 seconds, 138.41 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:30.245\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 88.29 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:30.245\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 3.33 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:30.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 31/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:30.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 110])\u001b[0m\n",
      "  2%|▉                                      | 135/5472 [00:00<00:27, 191.93it/s]\n",
      "\u001b[32m2025-06-20 06:29:31.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 137 tokens in 0.85 seconds, 160.98 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:31.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 102.69 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:31.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 3.33 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:31.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 32/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:31.099\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 136])\u001b[0m\n",
      "  2%|▌                                       | 80/5321 [00:00<00:26, 199.19it/s]\n",
      "\u001b[32m2025-06-20 06:29:31.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 82 tokens in 0.57 seconds, 144.12 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:31.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 91.94 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:31.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 3.33 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:31.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 33/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:31.670\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 81])\u001b[0m\n",
      "  3%|█▏                                     | 159/5224 [00:00<00:25, 202.27it/s]\n",
      "\u001b[32m2025-06-20 06:29:32.625\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 161 tokens in 0.96 seconds, 168.40 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:32.625\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 107.43 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:32.625\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 3.33 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:32.626\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 34/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:32.627\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 160])\u001b[0m\n",
      "  4%|█▍                                     | 188/5124 [00:00<00:24, 200.81it/s]\n",
      "\u001b[32m2025-06-20 06:29:33.713\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 190 tokens in 1.09 seconds, 174.90 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:33.713\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 111.57 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:33.713\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 3.33 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:33.713\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 35/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:33.715\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 189])\u001b[0m\n",
      " 10%|███▊                                   | 688/7062 [00:03<00:31, 201.58it/s]\n",
      "\u001b[32m2025-06-20 06:29:37.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 690 tokens in 3.54 seconds, 195.13 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:37.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 124.48 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:37.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 3.33 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:37.251\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 36/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:37.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 689])\u001b[0m\n",
      " 51%|███████████████████▍                  | 3254/6355 [00:16<00:15, 200.00it/s]\n",
      "\u001b[32m2025-06-20 06:29:53.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 3256 tokens in 16.44 seconds, 198.07 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:53.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 126.35 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:53.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 3.33 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:53.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 37/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:53.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 3255])\u001b[0m\n",
      "  0%|                                        | 14/7986 [00:00<00:42, 186.99it/s]\n",
      "\u001b[32m2025-06-20 06:29:54.176\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 16 tokens in 0.48 seconds, 33.04 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:54.176\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 21.08 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:54.176\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 6.81 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:54.176\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 38/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:54.177\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 15])\u001b[0m\n",
      "  0%|                                        | 13/7958 [00:00<00:42, 185.52it/s]\n",
      "\u001b[32m2025-06-20 06:29:54.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 15 tokens in 0.18 seconds, 84.84 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:54.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 54.12 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:54.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 6.81 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:54.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 39/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:54.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 14])\u001b[0m\n",
      "  0%|                                        | 19/7926 [00:00<00:41, 192.16it/s]\n",
      "\u001b[32m2025-06-20 06:29:54.560\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 21 tokens in 0.20 seconds, 102.53 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:54.560\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 65.40 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:54.560\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 6.81 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:54.560\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 40/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:54.561\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 20])\u001b[0m\n",
      "  0%|                                        | 15/7890 [00:00<00:42, 183.21it/s]\n",
      "\u001b[32m2025-06-20 06:29:54.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 17 tokens in 0.20 seconds, 82.94 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:54.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 52.91 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:54.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 6.81 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:54.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 41/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:54.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 16])\u001b[0m\n",
      "  0%|                                        | 18/7860 [00:00<00:43, 180.60it/s]\n",
      "\u001b[32m2025-06-20 06:29:54.985\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 20 tokens in 0.22 seconds, 91.73 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:54.986\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 58.52 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:54.986\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 6.81 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:54.986\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 42/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:54.987\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 19])\u001b[0m\n",
      "  0%|                                        | 15/7822 [00:00<00:42, 183.33it/s]\n",
      "\u001b[32m2025-06-20 06:29:55.189\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 17 tokens in 0.20 seconds, 83.92 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:55.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 53.53 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:55.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 6.81 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:55.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 43/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:55.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 16])\u001b[0m\n",
      "  0%|                                        | 13/7786 [00:00<00:42, 182.66it/s]\n",
      "\u001b[32m2025-06-20 06:29:55.373\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 15 tokens in 0.18 seconds, 82.39 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:55.373\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 52.56 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:55.373\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 6.81 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:55.374\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 44/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:55.374\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 14])\u001b[0m\n",
      "  0%|                                        | 11/7760 [00:00<00:44, 173.60it/s]\n",
      "\u001b[32m2025-06-20 06:29:55.549\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 13 tokens in 0.18 seconds, 74.26 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:55.549\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 47.37 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:55.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 6.81 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:55.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 45/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:55.551\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 12])\u001b[0m\n",
      "  0%|                                        | 15/7730 [00:00<00:41, 187.81it/s]\n",
      "\u001b[32m2025-06-20 06:29:55.753\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 17 tokens in 0.20 seconds, 83.85 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:55.754\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 53.49 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:55.754\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 6.81 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:55.754\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 46/46 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:55.755\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 16])\u001b[0m\n",
      "  0%|                                        | 15/7696 [00:00<00:41, 183.39it/s]\n",
      "\u001b[32m2025-06-20 06:29:55.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 17 tokens in 0.19 seconds, 90.05 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:55.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 57.45 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:55.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 6.81 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:29:55.945\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 16])\u001b[0m\n",
      "\u001b[32m2025-06-20 06:30:16.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.reference_loader\u001b[0m:\u001b[36mload_by_hash\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mUse same references\u001b[0m\n",
      "\u001b[32m2025-06-20 06:30:16.081\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: नोब्रोकर से घर ढूंढना इतना आसान और सुविधाजनक है, क्योंकि यहाँ आपको कोई\u001b[0m\n",
      "\u001b[32m2025-06-20 06:30:16.087\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: ब्रोकरेज फीस नहीं देनी पड़ती और आप सीधे मकान मालिक से बात कर सकते हैं,\u001b[0m\n",
      "\u001b[32m2025-06-20 06:30:16.093\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: जिससे आपका समय और पैसा दोनों बचता है।\u001b[0m\n",
      "\u001b[32m2025-06-20 06:30:16.093\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/3 of sample 1/1\u001b[0m\n",
      "  2%|▋                                      | 128/7898 [00:00<00:40, 193.78it/s]\n",
      "\u001b[32m2025-06-20 06:30:16.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 0.75 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 06:30:16.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 130 tokens in 0.75 seconds, 173.22 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:30:16.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 110.50 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:30:16.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 6.81 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:30:16.845\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/3 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:30:16.846\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 129])\u001b[0m\n",
      "  2%|▌                                      | 121/7621 [00:00<00:37, 197.86it/s]\n",
      "\u001b[32m2025-06-20 06:30:17.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 123 tokens in 0.74 seconds, 165.58 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:30:17.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 105.62 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:30:17.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 6.81 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:30:17.589\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/3 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:30:17.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 122])\u001b[0m\n",
      "  1%|▎                                       | 66/7416 [00:00<00:38, 189.70it/s]\n",
      "\u001b[32m2025-06-20 06:30:18.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 68 tokens in 0.48 seconds, 142.29 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:30:18.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 90.77 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:30:18.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 6.81 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:30:18.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 67])\u001b[0m\n",
      "\u001b[32m2025-06-20 06:31:15.075\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.reference_loader\u001b[0m:\u001b[36mload_by_hash\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mUse same references\u001b[0m\n",
      "\u001b[32m2025-06-20 06:31:15.095\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: नोब्रोकरवर घर शोधणे खूप सोपे आणि सोयीस्कर आहे, कारण इथे तुम्हाला कोणतेही\u001b[0m\n",
      "\u001b[32m2025-06-20 06:31:15.100\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: ब्रोकरेज फीस द्यावे लागत नाही आणि तुम्ही थेट मालकाशी संपर्क साधू शकता,\u001b[0m\n",
      "\u001b[32m2025-06-20 06:31:15.104\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: ज्यामुळे तुमचा वेळ आणि पैसा दोन्ही वाचतात.\u001b[0m\n",
      "\u001b[32m2025-06-20 06:31:15.105\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/3 of sample 1/1\u001b[0m\n",
      "  2%|▋                                      | 128/7886 [00:00<00:40, 189.25it/s]\n",
      "\u001b[32m2025-06-20 06:31:15.877\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 0.77 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 06:31:15.878\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 130 tokens in 0.77 seconds, 168.24 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:31:15.878\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 107.33 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:31:15.878\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 6.81 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:31:15.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/3 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:31:15.880\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 129])\u001b[0m\n",
      "  2%|▋                                      | 122/7605 [00:00<00:38, 196.66it/s]\n",
      "\u001b[32m2025-06-20 06:31:16.627\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 124 tokens in 0.75 seconds, 165.92 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:31:16.627\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 105.84 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:31:16.627\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 6.81 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:31:16.628\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/3 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:31:16.629\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 123])\u001b[0m\n",
      "  1%|▍                                       | 71/7386 [00:00<00:36, 198.31it/s]\n",
      "\u001b[32m2025-06-20 06:31:17.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 73 tokens in 0.52 seconds, 139.67 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:31:17.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 89.10 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:31:17.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 6.81 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:31:17.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 72])\u001b[0m\n",
      "\u001b[32m2025-06-20 06:32:13.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.reference_loader\u001b[0m:\u001b[36mload_by_hash\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mUse same references\u001b[0m\n",
      "\u001b[32m2025-06-20 06:32:13.960\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: नोब्रोकरवर घर शोधणे खूप सोपे आणि सोयीस्कर आहे, कारण इथे तुम्हाला कोणतेही\u001b[0m\n",
      "\u001b[32m2025-06-20 06:32:13.966\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: ब्रोकरेज फीस द्यावे लागत नाही आणि तुम्ही थेट मालकाशी संपर्क साधू शकता,\u001b[0m\n",
      "\u001b[32m2025-06-20 06:32:13.972\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: ज्यामुळे तुमचा वेळ आणि पैसा दोन्ही वाचतात.\u001b[0m\n",
      "\u001b[32m2025-06-20 06:32:13.972\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/3 of sample 1/1\u001b[0m\n",
      "  2%|▋                                      | 136/7886 [00:00<00:40, 190.02it/s]\n",
      "\u001b[32m2025-06-20 06:32:14.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 0.81 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 06:32:14.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 138 tokens in 0.81 seconds, 169.95 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:32:14.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 108.42 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:32:14.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 6.81 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:32:14.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/3 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:32:14.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 137])\u001b[0m\n",
      "  2%|▊                                      | 150/7597 [00:00<00:39, 189.97it/s]\n",
      "\u001b[32m2025-06-20 06:32:15.716\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 152 tokens in 0.93 seconds, 163.41 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:32:15.716\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 104.24 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:32:15.717\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 6.81 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:32:15.717\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/3 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:32:15.718\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 151])\u001b[0m\n",
      "  1%|▎                                       | 61/7350 [00:00<00:38, 189.86it/s]\n",
      "\u001b[32m2025-06-20 06:32:16.178\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 63 tokens in 0.46 seconds, 136.79 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:32:16.178\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 87.26 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:32:16.179\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 6.81 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:32:16.179\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 62])\u001b[0m\n",
      "\u001b[32m2025-06-20 06:33:51.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.reference_loader\u001b[0m:\u001b[36mload_by_hash\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mUse same references\u001b[0m\n",
      "\u001b[32m2025-06-20 06:33:51.864\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Finding a house on NoBroker is incredibly easy and convenient because you don't have to pay any brokerage fees, and you can directly connect with property owners,\u001b[0m\n",
      "\u001b[32m2025-06-20 06:33:51.870\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: which saves you both time and money while ensuring a transparent and hassle-free house hunting experience.\u001b[0m\n",
      "\u001b[32m2025-06-20 06:33:51.870\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/2 of sample 1/1\u001b[0m\n",
      "  2%|▊                                      | 161/8002 [00:00<00:46, 167.49it/s]\n",
      "\u001b[32m2025-06-20 06:33:52.947\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 1.08 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 06:33:52.948\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 163 tokens in 1.08 seconds, 151.31 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:33:52.948\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 96.53 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:33:52.948\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 6.81 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:33:52.949\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/2 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:33:52.949\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 162])\u001b[0m\n",
      "  2%|▋                                      | 129/7809 [00:00<00:45, 169.28it/s]\n",
      "\u001b[32m2025-06-20 06:33:53.875\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 131 tokens in 0.93 seconds, 141.46 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:33:53.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 90.24 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:33:53.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 6.81 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:33:53.877\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 130])\u001b[0m\n",
      "\u001b[32m2025-06-20 06:34:56.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.reference_loader\u001b[0m:\u001b[36mload_by_hash\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mUse same references\u001b[0m\n",
      "\u001b[32m2025-06-20 06:34:56.499\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Finding a house on NoBroker is incredibly easy and convenient because you don't have to pay any brokerage fees, and you can directly connect with property owners,\u001b[0m\n",
      "\u001b[32m2025-06-20 06:34:56.504\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: which saves you both time and money while ensuring a transparent and hassle-free house hunting experience.\u001b[0m\n",
      "\u001b[32m2025-06-20 06:34:56.504\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/2 of sample 1/1\u001b[0m\n",
      "  2%|▊                                      | 159/8002 [00:00<00:41, 190.58it/s]\n",
      "\u001b[32m2025-06-20 06:34:57.426\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 0.92 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 06:34:57.426\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 161 tokens in 0.92 seconds, 174.66 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:34:57.426\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 111.42 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:34:57.426\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 6.81 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:34:57.427\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/2 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-06-20 06:34:57.427\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 160])\u001b[0m\n",
      "  1%|▌                                      | 110/7811 [00:00<00:39, 195.50it/s]\n",
      "\u001b[32m2025-06-20 06:34:58.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 112 tokens in 0.69 seconds, 161.95 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:34:58.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 103.31 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:34:58.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 6.81 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:34:58.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 111])\u001b[0m\n",
      "\u001b[32m2025-06-20 06:36:10.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.reference_loader\u001b[0m:\u001b[36mload_by_hash\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mUse same references\u001b[0m\n",
      "\u001b[32m2025-06-20 06:36:10.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Finding a house on NoBroker is incredibly easy and convenient because you don't have to pay any brokerage fees.\u001b[0m\n",
      "\u001b[32m2025-06-20 06:36:10.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/1 of sample 1/1\u001b[0m\n",
      "  1%|▍                                      | 100/8011 [00:00<00:47, 165.93it/s]\n",
      "\u001b[32m2025-06-20 06:36:11.454\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 0.73 seconds\u001b[0m\n",
      "\u001b[32m2025-06-20 06:36:11.455\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 102 tokens in 0.73 seconds, 139.36 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-06-20 06:36:11.455\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 88.90 GB/s\u001b[0m\n",
      "\u001b[32m2025-06-20 06:36:11.455\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 6.81 GB\u001b[0m\n",
      "\u001b[32m2025-06-20 06:36:11.456\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 101])\u001b[0m\n",
      "Keyboard interruption in main thread... closing server.\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/envs/fish-speech/lib/python3.11/site-packages/gradio/blocks.py\", line 3090, in block_thread\n",
      "    time.sleep(0.1)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/Projects/nisarg/fish-speech/tools/run_webui.py\", line 104, in <module>\n",
      "    app.launch(show_api=True)\n",
      "  File \"/home/ubuntu/miniconda3/envs/fish-speech/lib/python3.11/site-packages/gradio/blocks.py\", line 2996, in launch\n",
      "    self.block_thread()\n",
      "  File \"/home/ubuntu/miniconda3/envs/fish-speech/lib/python3.11/site-packages/gradio/blocks.py\", line 3094, in block_thread\n",
      "    self.server.close()\n",
      "  File \"/home/ubuntu/miniconda3/envs/fish-speech/lib/python3.11/site-packages/gradio/http_server.py\", line 69, in close\n",
      "    self.thread.join(timeout=5)\n",
      "  File \"/home/ubuntu/miniconda3/envs/fish-speech/lib/python3.11/threading.py\", line 1123, in join\n",
      "    self._wait_for_tstate_lock(timeout=max(timeout, 0))\n",
      "  File \"/home/ubuntu/miniconda3/envs/fish-speech/lib/python3.11/threading.py\", line 1139, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python tools/run_webui.py \\\n",
    "    --llama-checkpoint-path checkpoints/fish-speech-1.5-SEP-TOKEN-lora \\\n",
    "    --decoder-checkpoint-path checkpoints/fish-speech-1.5/firefly-gan-vq-fsq-8x1024-21hz-generator.pth\\\n",
    "    --compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break-down CLI Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Encode reference audio: / 从语音生成 prompt: \n",
    "\n",
    "You should get a `fake.npy` file.\n",
    "\n",
    "你应该能得到一个 `fake.npy` 文件."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "## Enter the path to the audio file here\n",
    "src_audio = r\"D:\\PythonProject\\vo_hutao_draw_appear.wav\"\n",
    "\n",
    "!python fish_speech/models/vqgan/inference.py \\\n",
    "    -i {src_audio} \\\n",
    "    --checkpoint-path \"checkpoints/fish-speech-1.5/firefly-gan-vq-fsq-8x1024-21hz-generator.pth\"\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "audio = Audio(filename=\"fake.wav\")\n",
    "display(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate semantic tokens from text: / 从文本生成语义 token:\n",
    "\n",
    "> This command will create a codes_N file in the working directory, where N is an integer starting from 0.\n",
    "\n",
    "> You may want to use `--compile` to fuse CUDA kernels for faster inference (~30 tokens/second -> ~300 tokens/second).\n",
    "\n",
    "> 该命令会在工作目录下创建 codes_N 文件, 其中 N 是从 0 开始的整数.\n",
    "\n",
    "> 您可以使用 `--compile` 来融合 cuda 内核以实现更快的推理 (~30 tokens/秒 -> ~300 tokens/秒)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python fish_speech/models/text2semantic/inference.py \\\n",
    "    --text \"hello world\" \\\n",
    "    --prompt-text \"The text corresponding to reference audio\" \\\n",
    "    --prompt-tokens \"fake.npy\" \\\n",
    "    --checkpoint-path \"checkpoints/fish-speech-1.5\" \\\n",
    "    --num-samples 2\n",
    "    # --compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate speech from semantic tokens: / 从语义 token 生成人声:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python fish_speech/models/vqgan/inference.py \\\n",
    "    -i \"codes_0.npy\" \\\n",
    "    --checkpoint-path \"checkpoints/fish-speech-1.5/firefly-gan-vq-fsq-8x1024-21hz-generator.pth\"\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "audio = Audio(filename=\"fake.wav\")\n",
    "display(audio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fish-speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
